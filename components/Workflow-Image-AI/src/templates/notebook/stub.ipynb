{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "e134e05457d34029b6460cd73bbf1ed73f339b5b6d98c95be70b69eba114fe95"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Jupyter notebook to get started with a research PACS integration\n",
    "\n",
    "You may either edit the stub.py which is used inside the research PACS OR you can use this notebook to develop your solution. The last cell in this notebook will export its content into the stub.py script overwriting its content."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import glob\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import nbconvert\n",
    "\n",
    "# for testing inside the python notebook we need to set a fixed directory\n",
    "if not(os.path.isdir(sys.argv[1])):\n",
    "    # point the argument to one folder created with 'rpp trigger --keep'\n",
    "    sys.argv = [ \"python\", \"rpp_trigger_run_Friday_590776121\" ]\n",
    "\n",
    "description = {}\n",
    "with open(os.path.join(sys.argv[1], \"descr.json\")) as f:\n",
    "    # we might have more than one series as input, \n",
    "    # just use the first one for this example\n",
    "    description = json.load(f)[0]\n",
    "\n",
    "files = []\n",
    "print('glob: {}/input'.format(sys.argv[1]))\n",
    "for fname in glob.glob(sys.argv[1]+\"/input/*\", recursive=False):\n",
    "    #print(\"loading: {}\".format(fname))\n",
    "    if os.path.isfile(fname):\n",
    "        files.append(pydicom.dcmread(fname))\n",
    "\n",
    "# make sure we only keep data that has the same shape as the first slice\n",
    "files = [a for a in files if a.get(\"PixelData\") != None and a.pixel_array.shape == files[0].pixel_array.shape]\n",
    "\n",
    "print(\"file count: {}\".format(len(files)))\n",
    "\n",
    "# make sure we sort the slices by SliceLocation or, if that does not exist by InstanceNumber\n",
    "def sortFunc(s):\n",
    "    if \"SliceLocation\" in s:\n",
    "        return s.SliceLocation\n",
    "    else:\n",
    "        if \"InstanceNumber\" in s:\n",
    "            return s.InstanceNumber\n",
    "        return 0\n",
    "slices = sorted(files, key=sortFunc)\n",
    "\n",
    "# pixel aspects, assuming all slices are the same\n",
    "ps = slices[0].get(\"PixelSpacing\", [1,1])\n",
    "ss = slices[0].get(\"SliceThickness\",1)\n",
    "ax_aspect = ps[1]/ps[0]\n",
    "sag_aspect = ps[1]/ss\n",
    "cor_aspect = ps[0]/ss\n",
    "\n",
    "# create 3D array\n",
    "img_shape = list(slices[0].pixel_array.shape)\n",
    "img_shape.append(len(slices))\n",
    "img3d = np.zeros(img_shape)\n",
    "\n",
    "# fill 3D array with the images from the files\n",
    "for i, s in enumerate(slices):\n",
    "    img2d = s.pixel_array\n",
    "    img3d[:, :, i] = img2d\n",
    "\n",
    "# store any result DICOM data in sys.argv[1]/output\n",
    "output=os.path.join(sys.argv[1],\"output\")\n",
    "if not(os.path.exists(output)):\n",
    "    try:\n",
    "        os.mkdir(output,0o770)\n",
    "    except OSError as error:\n",
    "        print(error)"
   ]
  },
  {
   "source": [
    "We can plot the volume in three slice orientations to inspect the loaded data. This block uses matplotlib."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 3 orthogonal slices\n",
    "fig=plt.figure(figsize=(6,6))\n",
    "fig.patch.set_facecolor('gray')\n",
    "gs1 = gridspec.GridSpec(2,2)\n",
    "gs1.update(wspace=0.025, hspace=0.05)\n",
    "\n",
    "a1 = plt.subplot(gs1[0])\n",
    "plt.imshow(img3d[:, :, img_shape[2]//2], cmap='gray')\n",
    "a1.set_aspect(ax_aspect)\n",
    "a1.set_xticklabels([])\n",
    "\n",
    "a2 = plt.subplot(gs1[1])\n",
    "plt.imshow(img3d[:, img_shape[1]//2, :], cmap='gray')\n",
    "a2.set_aspect(sag_aspect)\n",
    "a2.set_xticklabels([])\n",
    "a2.set_yticklabels([])\n",
    "\n",
    "a3 = plt.subplot(gs1[2])\n",
    "plt.imshow(img3d[img_shape[0]//2, :, :], cmap='gray')\n",
    "a3.set_aspect(cor_aspect)\n",
    "a3.set_yticklabels([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Here might be a good place to add your work.\n",
    "\n",
    "- Volume: img3d\n",
    "- Structured Information: description\n",
    "\n",
    "As an example here we compute the signal-to-noise of img3d and safe the value to the output description file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = img3d.std()\n",
    "description['signal-to-noise'] = np.where(sd == 0, 0, img3d.mean()/sd).item()\n",
    "description['shape_x'] = img3d.shape[0]\n",
    "description['shape_y'] = img3d.shape[1]\n",
    "description['shape_z'] = img3d.shape[2]\n",
    "\n",
    "# remember to save the structured information into the output folder\n",
    "with open(output+\"/output.json\", 'w') as outfile:\n",
    "    outfile.write(json.dumps(description, indent=4, sort_keys=True))"
   ]
  },
  {
   "source": [
    "We need to convert this jupyther notebook to the python script to have it run inside the container environment (rpp build). This is done in the following block that depends on nbconvert."
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script stub.ipynb\n",
    "with open('stub.py', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "with open('stub.py', 'w') as f:\n",
    "    for line in lines:\n",
    "        if 'nbconvert --to script' in line:\n",
    "            break\n",
    "        else:\n",
    "            f.write(line)"
   ]
  }
 ]
}