# Specify a subset of the image series for processing

If your processing pipeline depends on specific image series you can filter out all other series. The ror program will only call your workflow with image series that match. There are two steps to create a filter. In a first step you can teach ror how to classify your image series. Afterwards you simply specify the class as a `--select`.

Basic classification information (classify rules) are added to the data description (descr.json) file as ClassifyTypes. This information comes from a .ror/classifyTypes.json file generated by ror during the init process. New classes for DICOM files can be added here. To explain the syntax lets look at the first type in the file called "GE":

```json
  {
    "type": "GE",
    "id": "GEBYMANUFACTURER",
    "description": "This scan is from GE",
    "rules": [
      {
        "tag": [
          "0x08",
          "0x70"
        ],
        "value": "^GE MEDICAL"
      }
    ]
  },
```

The class matches with any imaging studies from a General Electric (GE) scanner by checking if the value of the DICOM tag (0008,7000) matches with the regular expression "^GE MEDICAL" (starts with "GE MEDICAL"). Classifications can contain more than one matching tag (rules array). They can also contain rules that reference other rules. Here an example of a class that attempts to identify diffusion weighted image series from a Siemens scanner by scanning for 2 matching rules.

```json
  {
    "type": "DIFFUSION",
    "id": "DIFFUSION",
    "description": "SIEMENS diffusion weighted",
    "rules": [
      {
        "tag": [
          "SequenceName"
        ],
        "value": "*ep_b",
        "operator": "contains"
      },
      {
        "rule": "SIEMENSBYMANUFACTURER"
      }
    ]
  }
```

In general, classification rules will be site-based for many research projects. We might attempt to create a sufficiently large rule set to identify the default scan types from commercial vendors but any sequence programming will result in cases that might not be classified correctly using a given set of rules in classifyTypes.json.

### Simple glob-like series selection

To configure what image series are processed define a search filter like the following (all series with the DICOM tag SeriesNumber starting with "2")

```bash
ror config --select "SeriesNumber: 2"
```

This search text, a regular expression, is matched for each series against a string that contains

```bash
"StudyInstanceUID: %s, SeriesInstanceUID: %s, SeriesDescription: %s, NumImages: %d, SeriesNumber: %d, SequenceName: %s, Modality: %s, Manufacturer: %s, ManufacturerModelName: %s, StudyDescription: %s, ClassifyType: %s"
```

where ClassifyType is a comma separated array of detected classification types. To identify the diffusion scans from above the series filter could look like this (glob style filter):

```bash
ror config --select "ClassifyType: .*DIFFUSION"
```

### More complex input data selections using select

Analysis workflows might depend on more than an individual image series. If we do a longitudinal analysis all time points for a patient need to be available for analysis (patient level processing). This is also of interest if we require more than one image series, for example a fieldmap and a functional scan, or an anatomical T1 and a FLAIR scan from the same study (study level processing). The above glob-style filter will not work in these cases as it only provides a single matching image series as input to the workflow.

To generate sets of image data that are more complex than single specific image series a more complex selection language can be used. This language allows us to specify a unit of processing as complex as "a diffusion image series with a closest in time T1-weighted image series", or "all resting state image series with a suitable field map", or "all T1 weighted image series in the study from the first time point by patient, use the best quality scan if there is more than one for a patient". Based on the use-case we distinguish:

- Series level processing where a dataset contains all images of a DICOM series or volume
- Study level processing where a dataset contains all series of a single participant visit
- Patient level processing where a dataset contains all series for all studies for one participant
- Project level processing  where a dataset contains all data of a project

The later case is used to train an AI algorithm whereas the first and second are used for prediction. Patient level processing is suitable for longitudinal analysis.

You can use ror to suggest a selection on the series level. This call might take a long time if you have lots of data as it tries to randomly generate a 'good' selection that will create many series level datasets.

```bash
ror config --suggest
100/100 1.999
Suggested abstract syntax tree for your data [1.999383]
{
  "Output_level": "series",
  "Select_level": "series",
  "Select_level_by_rule": [
    "series"
  ],
  "Rule_list_names": [
    "no-name"
  ],
  "Rules": [
    [
      {
        "tag": [
          "Modality"
        ],
        "tag2": null,
        "value": "CR",
        "operator": "contains",
        "negate": "",
        "rule": ""
      }
    ]
  ],
  "CheckRules": null
}
To use this select statement call:
ror config --select '
SELECT series
  FROM study
    WHERE series NAMED "no-name" HAS
       Modality containing CR
'

We will run processing on any single image series that matches.
We will select cases with a single matching image series.

Given our current test data we can identify 9721 matching datasets.
```

In the above example the best rule found simply selects all series based on the modality (CR).

Here is a more complex example of a select statement. The syntax is similar to SQL, newlines and formatting are superfluous.

```bash
ror config --select '
Select patient
  from study
    where series named "T1" has
      ClassifyType containing T1 
    and 
      SeriesDescription regexp "^A" 
  also
    where series named "DIFF" has
      ClassifyType containing DIFFUSION
  also
    where series named "REST" has 
      ClassifyType containing RESTING 
    and 
      NumImages > 10  
    and 
      not(NumImages > 200)
'
```

The available logical operators on the 'where'-level include "and", "or", and "not". Use brackets '(' and ')' to enforce a grouping order.

### Details on select as a language to specify input datasets

The selection (domain specific) language first specifies a level at which the data is exported ('Select patient'). If processing depends on a single series only a 'Select series' will export a single random image series. If 'Select study' is used (default) all matching series of a study are exported. The 'from study' is not functional at the moment. In the future it is supposed to allow a construct like 'from earliest PROJECT_NAME by StudyDate as DICOM'. The third part is a list of where clauses delimited by 'also where series has' to separate selection rules for different series like one for a field map and another for a resting state scan. A where clause selecting a series can be named using the optional "named SOMENAME". This name will be available to the workflow to help identify the individual image series types. Each where clause is a list of rules that use the tags available for each series (ror status). Only tags from 'ror status' work. If a new tag needs to be included, which is not yet part of the series information provided by 'ror status' add the tag first to a new classify rule. Afterwards the new tag referencing that rule appears in ClassifyTypes and can be used in select (`ClassifyType containing <new type>`).

The possible syntax for rules is:

- `<field> containing <string>` check if the field list contains the specified string. A usual example is the field ClassifyTypes which is a list of types like "T1" or "DIFFUSION". The string needs to be in double quotes if it contains spaces.
- `<field> == <string>` compare if every entry of the field is truly equal to the specified string.
- `<field> < <num>` match if the field is a number field (SeriesNumber, NumImages) and smaller than the provided numeric value.
- `<field> > <num>` match if the field is a number field (SeriesNumber, NumImages) and larger than the provided numeric value.
- `<field> approx <string>` match if all entries in the field are numerically similar (1e-4) to the corresponding comma separated string values provided. This might not be very useful in the provided context but matches with the classifyRules.json definitions used for example for the detection of axial, sagittal and coronal scan orientations.
- `<field> regexp <string>` match the field with the provided regular expression. For example "^GE" would match with values that start with "GE", or "b$" matches with all strings that end with the letter "b", or "patient[6-9]" matches with all strings that have a 6, 7, 8, or 9 after "patient".

where `<field>` can be any of the following `[SeriesDescription|NumImages|SeriesNumber|SequenceName|Modality|StudyDescription|Manufacturer|ManufacturerModelName|PatientID|PatientName|ClassifyTypes]`.
Reference any other DICOM tag using the '("0x0000","0x0000")' notation for group and tag. The supported tags include all tags that have a value representation that is not array or binary.

### Select use-case: training a model

In order to train a model access to all the data is required. That means that the selection level has to be 'project'. Define a filter with

```bash
ror config --select '
  Select project       /* export level for all data in the study */
    from study         /* not functional */
      where series has /* start of a rule set */
        Modality = CT  /* selection rule */
'
```

This data selection will create at most one matching dataset, a single input folder with all (modality CT) image series for all participants and studies in the project.

### Select use-case: prediction on a single matching image series

Selection for individual image series should be done on the 'series' level with 'Select series ...'.

```bash
ror config --select '
  Select series       /* export level for a single image series */
    from study        /* not functional */
      where series has  /* start of a rule set */
        Modality = CT   /* selection rule */
      and
        NumImages > 100 /* numeric rule for number of slices in series */
'
```

### Select use-case: dependencies between series in a study

If 2 selected series in a study should share the same FrameOfReferenceUID (can be fused without registration) such a check can be enforced by adding a 'CHECK' section with rules that reference the named series from Select.

```bash
ror config  --select '
  Select study 
    from project
      where series named COR has
        ClassifyType containing coronal
    also
      where series named AX has
        ClassifyType containing axial
  Check
      COR@FrameOfReferenceUID = AX@FrameOfReferenceUID
    AND
      COR@Modality = AX@Modality
'
```

### Using select in the workflow

For select all image series that match are in a pool of potential datasets for the 'trigger' command. There is the option to run a single random dataset of them with `ror trigger`, or to run all of the possible datasets in a row with `ror trigger --each`. For testing of workflows it is suggested to start with:

```bash
ror trigger --keep
```

which will keep the input data to the workflow around after the trigger has finished. By default (without '--keep') all data folders are deleted after a single run. Fix any problems found with your workflow given that dataset. Do not change the content of the data folder only adjust your program. To test if your updated workflow works on the last dataset use

```bash
ror trigger --last
```

Once you workflow seems ok test with another random dataset using `ror trigger --keep` or simply try to run the workflow on all datasets with `ror trigger --each`.

## Output file format

The output folder and output.json file are parsed by the Research Information System after the run on the data in input. Research studies can have thousands of data objects that all can be processed in parallel. In order to provide a unifying view of the resulting individual result data objects, the Research Information System supports the storage of such information into a centralized database like REDCap. The only requirement for such storage is that individual results are annotated in a structured way. For example, we compute a signal-to-noise value for a given input folder. In order to store this single value we need to collect the following information:

```python
[{
    "record_id":  description["PatientID"],
    "redcap_event_name": description["ReferringPhysician"],
    "field_name": "signal-to-noise",
    "value":      np.where(sd == 0, 0, img3d.mean()/sd).item(),
}]
```

where 'record_id' identifies the participant name, 'event_name' identifies a timepoint, 'field_name' the variable that should be used to store the value and 'value' the actual computed entry.

The above structure corresponds to REDCap's data model for longitudinal event-related studies.
